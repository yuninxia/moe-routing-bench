mkdir -p checkpoints log
CUDA_VISIBLE_DEVICES=0 PYTHONPATH=$(pwd) python commonsense/finetune.py \
  --base_model allenai/OLMoE-1B-7B-0924 \
  --data_path commonsense/commonsense_170k.json \
  --output_dir checkpoints/OLMoE-1B-7B-0924.top2of4_r16 \
  --batch_size 16 --micro_batch_size 4 \
  --num_epochs 1 --learning_rate 1e-5 \
  --cutoff_len 256 --val_set_size 120 \
  --eval_step 80 --save_step 80 \
  --shared_routing_adapter True \
  --shared_routing_adapter_num_experts 4 \
  --shared_routing_adapter_num_experts_per_tok 2 \
  --adapter_type LoRA --lora_r 16 --lora_alpha 32 --dropout 0.05